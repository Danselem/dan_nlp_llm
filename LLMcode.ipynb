{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b11b26db-2ea6-4280-a648-52d0c5d96e7b",
   "metadata": {},
   "source": [
    "# Personal Generative AI with LLM Learning\n",
    "\n",
    "## 1 Dependencies\n",
    "\n",
    "- langchain\n",
    "- huggingface_hub\n",
    "- sentence_transformers\n",
    "- faiss-cpu\n",
    "- unstructured\n",
    "- youtube_transcript_api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3409a776-5bb6-4f40-9751-d316649c89ae",
   "metadata": {},
   "source": [
    "## 2 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ed8cfad-8e90-436e-a2d3-49b675363129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.document_loaders import TextLoader, UnstructuredPDFLoader, ArxivLoader, SeleniumURLLoader, OnlinePDFLoader\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdb49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hf_api.txt') as f:\n",
    "    hf_key = f.readlines()\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "369930b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87b05718-d806-496e-88c2-4303d2ebff63",
   "metadata": {},
   "source": [
    "## 3 Loading of Different Types of Document\n",
    "\n",
    "### 3.1 Loding Local Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20ea47c-5005-435c-9f18-828a29080b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadlocaltxtfile(txtfile):\n",
    "\n",
    "    # Load the text document using TextLoader\n",
    "    loader = TextLoader('./'+txtfile)\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"local_text_file.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cccecbc6-8bca-4c0b-b75c-2a63b1caf359",
   "metadata": {},
   "source": [
    "### 3.2 Loading Text from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915dd66f-75bb-4c77-b26a-74773465a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadtxtfromURL(text_url, output_txt_name):\n",
    "   # Fetching the text file\n",
    "    \n",
    "   response = requests.get(text_url)\n",
    "   with open(output_txt_name, \"w\",  encoding='utf-8') as file:\n",
    "      file.write(response.text)\n",
    "\n",
    "   # Load the text document using TextLoader\n",
    "   loader = TextLoader('./'+output_txt_name)\n",
    "   loaded_docs = loader.load()\n",
    "   return loaded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b707c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://raw.githubusercontent.com/vashAI/AnsweringQuestionsWithHuggingFaceAndLLM/main/url_text_file.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "090cedd5-54ce-4750-93d9-4e5482d4a33d",
   "metadata": {},
   "source": [
    "### 3.3 Loading Local PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5805a745-50fb-4513-b66e-49573649ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadlocalPDF(pdf_file):\n",
    "    loader = UnstructuredPDFLoader(pdf_file)\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909dd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Eurovision_Song_Contest_2023.pdf\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28397ce0-d1ef-49d0-85fd-918665439267",
   "metadata": {},
   "source": [
    "### 3.4 Loading Text from Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed8976b4-4e69-4f55-b3eb-c3db943b3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadwebsitetext(url):    # loader = UnstructuredURLLoader(urls=[url])\n",
    "    loader = SeleniumURLLoader(urls=[url])\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://saturncloud.io/blog/breaking-the-data-barrier-how-zero-shot-one-shot-and-few-shot-learning-are-transforming-machine-learning/\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e07c15cf-1574-4441-9391-0f26b8ede1e2",
   "metadata": {},
   "source": [
    "### 3.5 Reading text from YouTude Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922e4082-c2a3-4883-8a49-d27767bbc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadyoutubetext(youtube_video_id=\"eg9qDjws_bU\"):\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(youtube_video_id)\n",
    "\n",
    "    transcript_text = \"\"\n",
    "    for entry in transcript:\n",
    "        transcript_text += ' ' + entry['text']\n",
    "    \n",
    "    youtube_txt_file = \"youtube_transcript.txt\"\n",
    "    with open('./'+youtube_txt_file, \"w\",  encoding='utf-8') as file:\n",
    "      file.write(transcript_text)\n",
    "\n",
    "    # Load the text document using TextLoader\n",
    "    loader = TextLoader('./'+youtube_txt_file)\n",
    "    loaded_docs = loader.load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cff40752",
   "metadata": {},
   "source": [
    "### 3.6 Loading Arxiv Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68cd48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadtextfromArxiv(query):\n",
    "    loaded_docs = ArxivLoader(query=query, load_max_docs=5).load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62bf9f78",
   "metadata": {},
   "source": [
    "### 3.7 Loading Online PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88f75238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadonlinePDF(pdf_url):\n",
    "    loaded_docs = OnlinePDFLoader(pdf_url).load()\n",
    "    return loaded_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "713c7310-c185-42ba-8c69-de4a55ebdd20",
   "metadata": {},
   "source": [
    "## 4 Preprocessing\n",
    "\n",
    "### 4.1 Splitting documents in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa9aef4a-9bb4-49e4-97ce-9eb3ea356290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentsplitter(loaded_docs):\n",
    "    # Splitting documents into chunks\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_docs = splitter.split_documents(loaded_docs)\n",
    "    return chunked_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f333094-c0bb-4117-910c-fa3dc16f7eb7",
   "metadata": {},
   "source": [
    "### 4.2 Convert Documents to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd057fc0-70b0-4ef6-8018-6136fe6c8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddings(chunked_docs):\n",
    "    # Create embeddings and store them in a FAISS vector store\n",
    "    embedder = HuggingFaceEmbeddings()\n",
    "    vector_store = FAISS.from_documents(chunked_docs, embedder)\n",
    "    return vector_store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37754d9c-ee8c-4d08-82b7-846202e75cba",
   "metadata": {},
   "source": [
    "## 5 Building the Model\n",
    "### 5.1 Use embeddings to feed the LLM model and Answer Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e492b83-9d1e-48dc-abc2-86638f1f9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLLMModel():\n",
    "    llm=HuggingFaceHub(repo_id=\"declare-lab/flan-alpaca-large\", model_kwargs={\"temperature\":0.0, \"max_length\":512})\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    return chain\n",
    "\n",
    "def askQuestions(vector_store, chain, question):\n",
    "    # Ask a question using the QA chain\n",
    "    similar_docs = vector_store.similarity_search(question)\n",
    "    response = chain.run(input_documents=similar_docs, question=question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "861d5dc9-3345-41b5-ab89-fa5110a84a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = loadLLMModel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21adbcda-48d7-4404-9812-08aa7985fc0a",
   "metadata": {},
   "source": [
    "### 5.2 Test with Local file & Test with file from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05458723-fd52-4901-90b3-4be88efc7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_loaded_docs = loadlocaltxtfile('local_text_file.txt')\n",
    "local_chunked_docs = documentsplitter(local_loaded_docs)\n",
    "local_vector_store = createEmbeddings(local_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf93661-aafd-40b6-913c-7fc273fa22f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT and plugins are helping Citizen Data Scientists by providing them with the tools they need to analyze\n",
      "and interpret data. By enabling them to use natural language, they are able to ask questions and get answers\n",
      "in plain English, without knowing complex programming languages or statistical techniques. Additionally,\n",
      "ChatGPT is a personal expert who is always available to help them turn their idea into reality.\n"
     ]
    }
   ],
   "source": [
    "local_response = askQuestions(local_vector_store, chain, \"Explain me how ChatGPT and Plugin are empowering Citizen Data Scientists?\")\n",
    "print(wrap_text_preserve_newlines(local_response))\n",
    "# print(LOCAL_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa0eae1e-0fb4-4462-85e6-d8d963e3b12d",
   "metadata": {},
   "source": [
    "### 5.3 Test file from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "513281b6-9849-4776-bcb1-eae6cdcc4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_loaded_docs = loadtxtfromURL(\"https://raw.githubusercontent.com/vashAI/AnsweringQuestionsWithHuggingFaceAndLLM/main/url_text_file.txt\",\n",
    "                                  \"urltext.txt\")\n",
    "url_chunked_docs = documentsplitter(url_loaded_docs)\n",
    "url_vector_store = createEmbeddings(url_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da9f68b3-4b26-463f-90de-3615445cf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data visualization and analysis using ChatGPT and Plugins 2. Content creation and summarization 3.\n",
      "Personalized learning and skill development 4. Collaboration and knowledge sharing\n"
     ]
    }
   ],
   "source": [
    "url_response = askQuestions(url_vector_store, chain, \"What are 5 examples of chatgpt and plugin applications?\")\n",
    "print(wrap_text_preserve_newlines(url_response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c417bb78-9288-4b4b-9508-4ca5d3e9118d",
   "metadata": {},
   "source": [
    "### 5.4 Test with local PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8d35796-05d7-45d8-9736-3134c78adb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1019, which is longer than the specified 1000\n",
      "Created a chunk of size 1316, which is longer than the specified 1000\n",
      "Created a chunk of size 1425, which is longer than the specified 1000\n",
      "Created a chunk of size 1352, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "pdf_loaded_docs = loadlocalPDF(pdf_file=\"Eurovision_Song_Contest_2023.pdf\")\n",
    "pdf_chunked_docs = documentsplitter(pdf_loaded_docs)\n",
    "pdf_vector_store = createEmbeddings(pdf_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14452aec-f775-449b-b245-826025e0de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2023 Eurovision Songcontest didn't hold in Ukraine due to security concerns caused by the Russian invasion\n",
      "of Ukraine.\n"
     ]
    }
   ],
   "source": [
    "pdf_response = askQuestions(pdf_vector_store, chain, \"Why is it that the 2023 Eurovision Songcontest didn't hold in Ukraine?\")\n",
    "print(wrap_text_preserve_newlines(pdf_response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80f426c1-b5fc-4348-9516-9d9923a3832d",
   "metadata": {},
   "source": [
    "### 5.5 Test with Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8d9a41-b15d-4a9b-aad5-de18a7815007",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_url = \"https://deepchecks.com/glossary/zero-shot-learning/\"\n",
    "web_loaded_docs = loadwebsitetext(web_url)\n",
    "web_chunked_docs = documentsplitter(web_loaded_docs)\n",
    "web_vector_store = createEmbeddings(web_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c10d983-72b9-4c99-a473-5fb840d23993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot learning is a machine learning approach that uses a labeled training set of seen classes and unseen\n",
      "classes to build models for classes that have not yet been labeled for training. It transfers information from\n",
      "source classes to labeled samples using class properties as a part of information.\n"
     ]
    }
   ],
   "source": [
    "web_response = askQuestions(web_vector_store, chain, \"What is Zero-shot learning?\")\n",
    "print(wrap_text_preserve_newlines(web_response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96c417c-cc60-4e71-ac34-ba2183c0a370",
   "metadata": {},
   "source": [
    "### 5.6 Test with text from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd000306-fc8d-48f2-9fbd-f1b0674041b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_loaded_docs = loadyoutubetext(youtube_video_id=\"DIU48QL5Cyk\")\n",
    "vid_chunked_docs = documentsplitter(vid_loaded_docs)\n",
    "vid_vector_store = createEmbeddings(vid_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d264c4fe-5033-4935-bec6-197aca180ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trending LLMs are chat GPT, Google's Bard AI, and Adobe's AI art generator.\n"
     ]
    }
   ],
   "source": [
    "vid_response = askQuestions(vid_vector_store, chain, \"What are the trending LLMs?\")\n",
    "print(wrap_text_preserve_newlines(vid_response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abaccf98",
   "metadata": {},
   "source": [
    "### 5.7 Test with Arxiv Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e32228e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arxiv_loaded_docs = loadtextfromArxiv(query=\"2104.12520\")\n",
    "Arxiv_chunked_docs = documentsplitter(Arxiv_loaded_docs)\n",
    "Arxiv_vector_store = createEmbeddings(Arxiv_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb422968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common radio sources are OB stars, Be stars, flares from M dwarfs, and Ultra Compact HII regions.\n"
     ]
    }
   ],
   "source": [
    "Arxiv_response = askQuestions(Arxiv_vector_store, chain, \"What are the common radio sources?\")\n",
    "print(wrap_text_preserve_newlines(Arxiv_response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb80aca",
   "metadata": {},
   "source": [
    "### 5.8 Test with Online PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65f327a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1152, which is longer than the specified 1000\n",
      "Created a chunk of size 1110, which is longer than the specified 1000\n",
      "Created a chunk of size 2650, which is longer than the specified 1000\n",
      "Created a chunk of size 1125, which is longer than the specified 1000\n",
      "Created a chunk of size 1058, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n",
      "Created a chunk of size 1169, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "opdf_loaded_docs = loadonlinePDF(pdf_url=\"https://arxiv.org/pdf/2104.12520.pdf\")\n",
    "opdf_chunked_docs = documentsplitter(opdf_loaded_docs)\n",
    "opdf_vector_store = createEmbeddings(opdf_chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46cb9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper provides a comprehensive analysis of the evolution of stars in a galaxy. It uses the Besançon model\n",
      "to predict the distribution of stars at each location in the galaxy, based on the thin disk, the thick disk,\n",
      "the halo, and the bulge. The model is gridded to a distance step size specified by the user and produces a\n",
      "table of stars with their parameter bases on the input selection provided by the user.\n"
     ]
    }
   ],
   "source": [
    "opdf_response = askQuestions(opdf_vector_store, chain, \"Summarize the paper?\")\n",
    "\n",
    "print(wrap_text_preserve_newlines(str(opdf_response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
